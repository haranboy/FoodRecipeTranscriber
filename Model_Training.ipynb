{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3drcVeTlfxK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qyPz7kXll4if"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/sample_data/MistralTraining')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "QAQCBrmANW0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correct Versions:**\n",
        "\n",
        "install fsspec==2025.3.0\n",
        "\n",
        "install datasets\n",
        "\n",
        "install torch==2.6.0\n",
        "\n",
        "install torchvision==0.21.0\n",
        "\n",
        "install torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "install transformers==4.51.0\n",
        "\n",
        "install --upgrade peft>=0.7.0\n",
        "\n",
        "install xformers==0.0.29.post3\n",
        "\n",
        "install protobuf>=4.25.2\n",
        "\n",
        "install --upgrade unsloth-zoo\n",
        "\n",
        "install --upgrade unsloth"
      ],
      "metadata": {
        "id": "TgiCOsi6O-eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fsspec==2025.3.2\n",
        "!pip install datasets==3.0.0\n",
        "!pip install torch==2.6.0\n",
        "!pip install torchvision==0.21.0\n",
        "!pip install torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install transformers==4.51.0\n",
        "!pip install --upgrade peft>=0.7.0\n",
        "!pip install xformers==0.0.29.post3\n",
        "!pip install protobuf>=4.25.2\n",
        "!pip install trl\n",
        "!pip install bitsandbytes\n",
        "!pip install --upgrade unsloth-zoo\n",
        "!pip install --upgrade unsloth"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2NgK7LHAOMf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85IBmEr6mlBU",
        "outputId": "eb0d129a-bdf6-437f-f4c5-e00210f81125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Mistraltrain` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Mistraltrain`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqQmZ_mrm064",
        "outputId": "5ace95a0-a4f2-410f-d66f-320506b7238f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-14 19:27:45.529478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752521265.549011    9592 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752521265.555075    9592 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-14 19:27:45.574728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading and formatting data...\n",
            "Training examples: 34\n",
            "Evaluation examples: 9\n",
            "\\nExample formatted data point:\n",
            "[INST] Generate the cooking steps for the following recipe:\\n\\nRecipe Title: No Title Provided\\nIngredients:\\nNo ingredients listed. [/INST] No steps provided.\n",
            "Loading base model mistralai/Mistral-7B-Instruct-v0.2 with 4-bit quantization...\n",
            "Loading checkpoint shards: 100% 3/3 [01:10<00:00, 23.49s/it]\n",
            "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n",
            "Adding EOS to train dataset: 100% 34/34 [00:00<00:00, 1389.29 examples/s]\n",
            "Tokenizing train dataset: 100% 34/34 [00:00<00:00, 1032.29 examples/s]\n",
            "Truncating train dataset: 100% 34/34 [00:00<00:00, 5146.94 examples/s]\n",
            "Adding EOS to eval dataset: 100% 9/9 [00:00<00:00, 4967.59 examples/s]\n",
            "Tokenizing eval dataset: 100% 9/9 [00:00<00:00, 2348.43 examples/s]\n",
            "Truncating eval dataset: 100% 9/9 [00:00<00:00, 5148.49 examples/s]\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "Starting training...\n",
            "  0% 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "{'loss': 0.8931, 'grad_norm': 0.022508971393108368, 'learning_rate': 0.00017774855506796496, 'num_tokens': 7650.0, 'mean_token_accuracy': 0.8892676830291748, 'epoch': 5.0}\n",
            "{'loss': 0.0119, 'grad_norm': 0.0026794085279107094, 'learning_rate': 0.00010808804403614043, 'num_tokens': 15300.0, 'mean_token_accuracy': 0.9978535374005636, 'epoch': 10.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0008099274127744138, 'learning_rate': 3.340847749883191e-05, 'num_tokens': 22950.0, 'mean_token_accuracy': 1.0, 'epoch': 15.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0006227499106898904, 'learning_rate': 5.2443095448506674e-08, 'num_tokens': 30600.0, 'mean_token_accuracy': 1.0, 'epoch': 20.0}\n",
            "100% 100/100 [15:11<00:00,  8.04s/it]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 9.739479537529405e-06, 'eval_runtime': 3.8074, 'eval_samples_per_second': 2.364, 'eval_steps_per_second': 0.525, 'eval_num_tokens': 30600.0, 'eval_mean_token_accuracy': 1.0, 'epoch': 20.0}\n",
            "100% 100/100 [15:15<00:00,  8.04s/it]\n",
            "100% 2/2 [00:01<00:00,  1.96it/s]\u001b[A\n",
            "{'train_runtime': 917.4951, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.109, 'train_loss': 0.22625097601267044, 'epoch': 20.0}\n",
            "100% 100/100 [15:17<00:00,  9.17s/it]\n",
            "Training complete. Saving LoRA adapters locally...\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "Merging LoRA adapters with the base model...\n",
            "Loading checkpoint shards: 100% 3/3 [01:10<00:00, 23.56s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "Merged 4-bit quantized model saved locally to 'mistral_7b_recipes_merged'.\n",
            "LoRA adapters should have been pushed to Hugging Face Hub automatically: huggingfaceworkspace/mistral-7b-recipes-quantized\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
